{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80f09e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import dvc.api\n",
    "import dagshub\n",
    "from types import SimpleNamespace\n",
    "from src.logger import ExecutorLogger\n",
    "from src.training.model_wrapper import ModelWrapper\n",
    "import litserve as ls\n",
    "from src.inference.requests import InferenceRequest\n",
    "from src.training.process_data import dtype_conversion  # must be imported before pickle.load\n",
    "import joblib\n",
    "# Utility to convert dicts to namespaces\n",
    "def dict_to_namespace(d):\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            d[k] = dict_to_namespace(v)\n",
    "    return SimpleNamespace(**d)\n",
    "# Get the absolute path to the project root\n",
    "PROJECT_ROOT = '.'\n",
    "\n",
    "# Construct full paths to the pickle files\n",
    "MODEL_PATH = os.path.join(PROJECT_ROOT, 'models', 'basemodel', 'final_model.pkl')\n",
    "TRANSLATOR_PATH = os.path.join(PROJECT_ROOT, 'models', 'basemodel', 'model_target_translator.pkl')\n",
    "PIPELINE_PATH    = os.path.join(PROJECT_ROOT, \"data\", \"processed\", \"pipeline.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b7ad466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL PATH EXISTS: True\n",
      "TRANSLATOR PATH EXISTS: True\n",
      "PIPELINE PATH EXISTS: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Validate file paths\n",
    "print(\"MODEL PATH EXISTS:\", os.path.exists(MODEL_PATH))\n",
    "print(\"TRANSLATOR PATH EXISTS:\", os.path.exists(TRANSLATOR_PATH))\n",
    "print(\"PIPELINE PATH EXISTS:\", os.path.exists(PIPELINE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69336bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model type: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "Pipeline loaded successfully!\n",
      "Pipeline steps: ['dtype_conversion', 'numeric_imputer', 'encoder', 'scaler', 'drop_features']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(MODEL_PATH, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(\"Model type:\", type(model))\n",
    "except Exception as e:\n",
    "    print(\"Model loading failed:\", e)\n",
    "try:\n",
    "    with open(PIPELINE_PATH, 'rb') as f:\n",
    "        pipeline = joblib.load(f)\n",
    "    print(\"Pipeline loaded successfully!\")\n",
    "    print(\"Pipeline steps:\", [step[0] for step in pipeline.steps])\n",
    "except Exception as e:\n",
    "    print(\"Pipeline loading failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d4b943c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translator loaded successfully!\n",
      "Translator structure: dict_keys(['encoder', 'decoder'])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(TRANSLATOR_PATH, 'rb') as f:\n",
    "        translator = pickle.load(f)\n",
    "    print(\"Translator loaded successfully!\")\n",
    "    print(\"Translator structure:\", translator.keys())\n",
    "except Exception as e:\n",
    "    print(\"Translator loading failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19f4267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference.app import InferenceAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41b64692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column validation failed: RUN_ID environment variable must be set.\n"
     ]
    }
   ],
   "source": [
    "# Check if raw columns are defined\n",
    "try:\n",
    "    test_api = InferenceAPI()\n",
    "    test_api.setup()\n",
    "    print(\"Raw columns:\", test_api._raw_cols)  # This will likely fail - see analysis below\n",
    "except Exception as e:\n",
    "    print(\"Column validation failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd071ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05204b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae9edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea0e306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from functools import partial\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL, Trials, fmin, hp, tpe\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from omegaconf import OmegaConf\n",
    "import dvc.api\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from src.logger import ExecutorLogger\n",
    "from types import SimpleNamespace\n",
    "import dagshub\n",
    "import mlflow\n",
    "\n",
    "from src.training.model_wrapper import ModelWrapper\n",
    "\n",
    "def dict_to_namespace(d):\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            d[k] = dict_to_namespace(v)\n",
    "    return SimpleNamespace(**d)\n",
    "\n",
    "def setup_mlflow(tracking_uri: str):\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    client = mlflow.client.MlflowClient(tracking_uri=tracking_uri)\n",
    "    return client\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_runID(logged_model):\n",
    "    load_dotenv(\".env\")\n",
    "    cfg = dvc.api.params_show()\n",
    "    cfg_model = dict_to_namespace(cfg[\"model\"])\n",
    "\n",
    "    cfg = cfg_model\n",
    "    model_name = cfg.model_name\n",
    "    MODEL_PATH = cfg.model_path\n",
    "    PROCESSED_PATH = cfg.processed_data_path\n",
    "    FILE_NAME = cfg.file_name\n",
    "    TARGET = cfg.target\n",
    "\n",
    "\n",
    "    df_test = pd.read_parquet(os.path.join(PROCESSED_PATH, f\"{FILE_NAME}-test.parquet\"))\n",
    "    X_test, y_test = df_test.drop(columns=[TARGET]), df_test[TARGET]\n",
    "\n",
    "\n",
    "    dagshub.auth.add_app_token(token=os.getenv(\"DAGSHUB_TOKEN\"))\n",
    "    dagshub.init(\n",
    "        repo_owner=os.getenv(\"DAGSHUB_USERNAME\"), \n",
    "        repo_name=cfg_model.repo_name, \n",
    "        mlflow=cfg_model.use_mlflow\n",
    "    )\n",
    "    client = setup_mlflow(cfg_model.tracking_uri)\n",
    "    return mlflow.pyfunc.load_model(logged_model)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6fcc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:dagshub.auth.tokens:The added token already exists in the token cache, skipping\n",
      "INFO:httpx:HTTP Request: GET https://dagshub.com/api/v1/repos/Abdelhakiem/MLOps-Tasks \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"Abdelhakiem/MLOps-Tasks\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"Abdelhakiem/MLOps-Tasks\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dagshub:Initialized MLflow to track repo \"Abdelhakiem/MLOps-Tasks\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository Abdelhakiem/MLOps-Tasks initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository Abdelhakiem/MLOps-Tasks initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dagshub:Repository Abdelhakiem/MLOps-Tasks initialized!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07175eec1f04f64afdc3d4bdc4d7b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "logged_model = 'runs:/5dd549707b534d61b618b80c10e3868a/LogisticRegression'\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = get_model_runID(logged_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbaadc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Predict on your data.\n",
    "data = X_test.copy()\n",
    "loaded_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d62c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8cc44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07c2973e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"titanic.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15b854a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = {'PassengerId':1 , \n",
    "'Pclass': 2, \n",
    "'Name': 'Abdo' , \n",
    "'Sex':'male' , \n",
    "'Age' : 22.0 , \n",
    "'SibSp': 1, \n",
    "'Parch': 0 ,\n",
    "'Ticket' : '347082', \n",
    "'Fare': 7.25, \n",
    "'Cabin': np.nan , \n",
    "'Embarked': 'S'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a24a65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from src.training.process_data import dtype_conversion  # used inside pipeline\n",
    "import joblib\n",
    "# Paths (adjust if needed)\n",
    "PROJECT_ROOT = \".\"\n",
    "MODEL_PATH       = os.path.join(PROJECT_ROOT, \"models\", \"basemodel\", \"final_model.pkl\")\n",
    "TRANSLATOR_PATH  = os.path.join(PROJECT_ROOT, \"models\", \"basemodel\", \"model_target_translator.pkl\")\n",
    "PIPELINE_PATH    = os.path.join(PROJECT_ROOT, \"data\", \"processed\", \"pipeline.pkl\")\n",
    "\n",
    "# 1. Load artifacts\n",
    "with open(MODEL_PATH, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "with open(TRANSLATOR_PATH, \"rb\") as f:\n",
    "    translator = pickle.load(f)\n",
    "with open(PIPELINE_PATH, \"rb\") as f:\n",
    "    pipeline  = joblib.load(PIPELINE_PATH)\n",
    "\n",
    "# 2. Create a raw‐data sample just like your original Titanic CSV\n",
    "def make_sample() -> pd.DataFrame:\n",
    "    return pd.DataFrame([{\n",
    "        \"PassengerId\": 999,\n",
    "        \"Survived\":    0,        # you’ll drop this before predict\n",
    "        \"Pclass\":      3,\n",
    "        \"Name\":        \"Test, Mr. Example\",\n",
    "        \"Sex\":         \"male\",\n",
    "        \"Age\":         29.0,\n",
    "        \"SibSp\":       1,\n",
    "        \"Parch\":       0,\n",
    "        \"Ticket\":      \"ABC 12345\",\n",
    "        \"Fare\":        30.5,\n",
    "        \"Cabin\":       None,\n",
    "        \"Embarked\":    \"S\",\n",
    "    }])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abda3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"input\": {\n",
    "    \"PassengerId\": 1,\n",
    "    \"Survived\": 0,\n",
    "    \"Pclass\": 3,\n",
    "    \"Name\": \"Braund, Mr. Owen Harris\",\n",
    "    \"Sex\": \"male\",\n",
    "    \"Age\": 22.0,\n",
    "    \"SibSp\": 1,\n",
    "    \"Parch\": 0,\n",
    "    \"Ticket\": \"A/5 21171\",\n",
    "    \"Fare\": 7.25,\n",
    "    \"Cabin\": null,\n",
    "    \"Embarked\": \"S\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3821c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator[\"decoder\"] = {0: \"did_not_survive\", 1: \"survived\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "430442f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric: [0] decoded: ['did_not_survive']\n"
     ]
    }
   ],
   "source": [
    "def encode_and_predict(raw_df: pd.DataFrame) -> np.ndarray:\n",
    "    processed = pipeline.transform(raw_df)\n",
    "    if \"Survived\" in processed.columns:\n",
    "        processed = processed.drop(columns=\"Survived\")\n",
    "    return model.predict(processed)\n",
    "\n",
    "def decode_output(numeric_preds: np.ndarray, decoder: dict[int, any]) -> list[any]:\n",
    "    decoded = []\n",
    "    for v in numeric_preds:\n",
    "        orig = decoder[int(v)]\n",
    "        if isinstance(orig, np.generic):\n",
    "            orig = orig.item()\n",
    "        decoded.append(orig)\n",
    "    return decoded\n",
    "\n",
    "# demo:\n",
    "raw = make_sample()           # one-row DataFrame\n",
    "y_num   = encode_and_predict(raw)\n",
    "y_label = decode_output(y_num, translator[\"decoder\"])\n",
    "print(\"numeric:\", y_num, \"decoded:\", y_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b138d77",
   "metadata": {},
   "source": [
    "# Create Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbfef8",
   "metadata": {},
   "source": [
    "### Processing and Encoding Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "df.set_index(\"PassengerId\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d919c3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>male</td>\n",
       "      <td>female</td>\n",
       "      <td>female</td>\n",
       "      <td>female</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>22.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticket</th>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>113803</td>\n",
       "      <td>373450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>7.25</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>7.925</td>\n",
       "      <td>53.1</td>\n",
       "      <td>8.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>NaN</td>\n",
       "      <td>C85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C123</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0  \\\n",
       "PassengerId                        1   \n",
       "Survived                           0   \n",
       "Pclass                             3   \n",
       "Name         Braund, Mr. Owen Harris   \n",
       "Sex                             male   \n",
       "Age                             22.0   \n",
       "SibSp                              1   \n",
       "Parch                              0   \n",
       "Ticket                     A/5 21171   \n",
       "Fare                            7.25   \n",
       "Cabin                            NaN   \n",
       "Embarked                           S   \n",
       "\n",
       "                                                             1  \\\n",
       "PassengerId                                                  2   \n",
       "Survived                                                     1   \n",
       "Pclass                                                       1   \n",
       "Name         Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "Sex                                                     female   \n",
       "Age                                                       38.0   \n",
       "SibSp                                                        1   \n",
       "Parch                                                        0   \n",
       "Ticket                                                PC 17599   \n",
       "Fare                                                   71.2833   \n",
       "Cabin                                                      C85   \n",
       "Embarked                                                     C   \n",
       "\n",
       "                                  2  \\\n",
       "PassengerId                       3   \n",
       "Survived                          1   \n",
       "Pclass                            3   \n",
       "Name         Heikkinen, Miss. Laina   \n",
       "Sex                          female   \n",
       "Age                            26.0   \n",
       "SibSp                             0   \n",
       "Parch                             0   \n",
       "Ticket             STON/O2. 3101282   \n",
       "Fare                          7.925   \n",
       "Cabin                           NaN   \n",
       "Embarked                          S   \n",
       "\n",
       "                                                        3  \\\n",
       "PassengerId                                             4   \n",
       "Survived                                                1   \n",
       "Pclass                                                  1   \n",
       "Name         Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "Sex                                                female   \n",
       "Age                                                  35.0   \n",
       "SibSp                                                   1   \n",
       "Parch                                                   0   \n",
       "Ticket                                             113803   \n",
       "Fare                                                 53.1   \n",
       "Cabin                                                C123   \n",
       "Embarked                                                S   \n",
       "\n",
       "                                    4  \n",
       "PassengerId                         5  \n",
       "Survived                            0  \n",
       "Pclass                              3  \n",
       "Name         Allen, Mr. William Henry  \n",
       "Sex                              male  \n",
       "Age                              35.0  \n",
       "SibSp                               0  \n",
       "Parch                               0  \n",
       "Ticket                         373450  \n",
       "Fare                             8.05  \n",
       "Cabin                             NaN  \n",
       "Embarked                            S  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe598185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3a0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3183a05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.PassengerId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834c387e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a88c5b5",
   "metadata": {},
   "source": [
    "Cleaning Pipeline:\n",
    "- Remove unnecessary columns\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21875df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0       3    male  22.0      1      0   7.2500        S\n",
       "1       1  female  38.0      1      0  71.2833        C\n",
       "2       3  female  26.0      0      0   7.9250        S\n",
       "3       1  female  35.0      1      0  53.1000        S\n",
       "4       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_features = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"Survived\"]\n",
    "reduced_df = df.drop(drop_features, axis=1)\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8436ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"Age\", \"Fare\", \"SibSp\", \"Parch\"]\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51e719c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "categorical_transfomer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94c5a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preporcessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features),\n",
    "        (\"drop\", \"drop\", drop_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db1d4401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.53037664, -0.50244517,  0.43279337, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.57183099,  0.78684529,  0.43279337, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.25482473, -0.48885426, -0.4745452 , ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [        nan, -0.17626324,  0.43279337, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.25482473, -0.04438104, -0.4745452 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.15850313, -0.49237783, -0.4745452 , ...,  1.        ,\n",
       "         0.        ,  0.        ]], shape=(891, 13))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preporcessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b77f7b",
   "metadata": {},
   "source": [
    "# Trining Pipelien\n",
    "### processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import joblib\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "# from feature_engine.imputation import MeanMedianImputer\n",
    "# from feature_engine.encoding import OneHotEncoder\n",
    "# from feature_engine.selection import DropFeatures\n",
    "# from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SOURCE = os.path.join(\"data\", \"raw\")\n",
    "# DESTINATION = os.path.join(\"data\", \"processed\")\n",
    "\n",
    "# def dtype_conversion(X, cat_cols):\n",
    "#     \"\"\"Handle missing values and convert to categorical\"\"\"\n",
    "#     X = X.copy()\n",
    "#     for col in cat_cols:\n",
    "#         if col in X.columns:\n",
    "#             # Fill NA and convert to category\n",
    "#             X[col] = X[col].fillna('missing').astype('category')\n",
    "#     return X\n",
    "# def read_process_data(\n",
    "#     file_name: str,\n",
    "#     target: str,\n",
    "#     num_cols: list,\n",
    "#     cat_cols: list,\n",
    "#     drop_cols: list,\n",
    "#     logger\n",
    "# ):\n",
    "#     \"\"\"Data processing pipeline\"\"\"\n",
    "#     logger.info(\"Starting data processing\")\n",
    "\n",
    "#     try:\n",
    "#         # 1. Load data\n",
    "#         df = pd.read_csv(os.path.join(SOURCE, f'{file_name}.csv'))\n",
    "#         logger.info(f\"Raw data loaded: {df.shape}\")\n",
    "\n",
    "#         # 2. Validate initial data\n",
    "#         if df[target].isna().any():\n",
    "#             raise ValueError(f\"Target column '{target}' contains missing values\")\n",
    "\n",
    "#         # 3. Split data\n",
    "#         train_df, test_df = train_test_split(\n",
    "#             df, test_size=0.2, random_state=42, stratify=df[target]\n",
    "#         )\n",
    "#         logger.info(f\"Train/Test split: {train_df.shape}/{test_df.shape}\")\n",
    "\n",
    "#         # 4. Create processing pipeline\n",
    "#         processing_pipeline = Pipeline([\n",
    "#             ('dtype_conversion', FunctionTransformer(\n",
    "#                 func=dtype_conversion,\n",
    "#                 kw_args={'cat_cols': cat_cols},\n",
    "#                 validate=False\n",
    "#             )),\n",
    "\n",
    "#             ('numeric_imputer', MeanMedianImputer(\n",
    "#                 imputation_method='median',\n",
    "#                 variables=num_cols\n",
    "#             )),\n",
    "\n",
    "#             ('encoder', OneHotEncoder(\n",
    "#                 drop_last=True,\n",
    "#                 variables=cat_cols\n",
    "#             )),\n",
    "\n",
    "#             ('scaler', SklearnTransformerWrapper(\n",
    "#                 transformer=StandardScaler(),\n",
    "#                 variables=num_cols\n",
    "#             )),\n",
    "\n",
    "#             ('drop_features', DropFeatures(\n",
    "#                 features_to_drop=drop_cols + [target]\n",
    "#             ))\n",
    "#         ])\n",
    "\n",
    "#         # 5. Process data\n",
    "#         X_train = processing_pipeline.fit_transform(train_df)\n",
    "#         X_test = processing_pipeline.transform(test_df)\n",
    "\n",
    "#         # 6. Combine with target (critical fix)\n",
    "#         train_clean = pd.concat([\n",
    "#             X_train,\n",
    "#             train_df[target].rename(target)  # Preserve original index\n",
    "#         ], axis=1)\n",
    "\n",
    "#         test_clean = pd.concat([\n",
    "#             X_test,\n",
    "#             test_df[target].rename(target)  # Preserve original index\n",
    "#         ], axis=1)\n",
    "\n",
    "#         # 7. Validate output\n",
    "#         if len(train_clean) != len(train_df):\n",
    "#             raise ValueError(\"Row count mismatch in training data\")\n",
    "#         if len(test_clean) != len(test_df):\n",
    "#             raise ValueError(\"Row count mismatch in test data\")\n",
    "\n",
    "#         # 8. Save artifacts\n",
    "#         os.makedirs(DESTINATION, exist_ok=True)\n",
    "#         train_clean.to_parquet(os.path.join(DESTINATION, f\"{file_name}-train.parquet\"))\n",
    "#         test_clean.to_parquet(os.path.join(DESTINATION, f\"{file_name}-test.parquet\"))\n",
    "#         joblib.dump(processing_pipeline, os.path.join(DESTINATION, \"pipeline.pkl\"))\n",
    "\n",
    "#         logger.info(f\"Processing complete. Final shapes: Train {train_clean.shape}, Test {test_clean.shape}\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Processing failed: {str(e)}\")\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf5812",
   "metadata": {},
   "source": [
    "### Training and Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# import pickle\n",
    "# SOURCE = os.path.join(\"data\", \"processed\")\n",
    "# MODEL_PATH = 'models'\n",
    "# def encode_target(file_name : str,\n",
    "#                   target_col : str,\n",
    "#                   model_name : str,\n",
    "#                   logger):\n",
    "#     df_train = pd.read_parquet(os.path.join(DESTINATION, f\"{file_name}-train.parquet\"))\n",
    "#     df_test = pd.read_parquet(os.path.join(DESTINATION, f\"{file_name}-test.parquet\"))\n",
    "#     X_train , y_train = df_train.drop(columns=[target_col],axis=1) , df_train[target_col]\n",
    "#     X_test ,y_test = df_test.drop(columns=[target_col],axis=1) , df_test[target_col]\n",
    "\n",
    "#     logger.info(\"Fitting the encoder/decoder of target variable\")\n",
    "#     logger.info(f\"Number of classes: {len(y_train.unique())}\")\n",
    "#     \"\"\"Create and fit encoder/decoder for target variable\"\"\"\n",
    "#     encoder = LabelEncoder()\n",
    "#     encoder.fit(y_train)\n",
    "#     # Create decoder mapping\n",
    "#     classes = encoder.classes_\n",
    "#     decoder = {i: cls for i, cls in enumerate(classes)}\n",
    "#     target_translator = {\n",
    "#         'encoder': encoder,\n",
    "#         'decoder': decoder,\n",
    "#     }\n",
    "#     logger.info(\"encoder/decoder of target created successfully\")\n",
    "#     # Save the artifacts\n",
    "\n",
    "#     if not os.path.exists(os.path.join(MODEL_PATH, model_name)):\n",
    "#         os.makedirs(os.path.join(MODEL_PATH, model_name))\n",
    "#     with open(\n",
    "#         os.path.join(MODEL_PATH, model_name, \"model_target_translator.pkl\"),\n",
    "#         \"wb\",\n",
    "#     ) as pkl:\n",
    "#         pickle.dump(target_translator, pkl)\n",
    "#     logger.info(\"encoder/decoder of target saved\")\n",
    "#     return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13dd08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "# import os\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "# from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "# from hyperopt.pyll import scope\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import cross_validate\n",
    "\n",
    "# N_FOLDS = 3\n",
    "# MAX_EVALS = 3\n",
    "\n",
    "# # Updated search space with compatible parameters\n",
    "# SPACE = {\n",
    "#     \"penalty\": hp.choice(\"penalty\", [\"l1\", \"l2\", \"elasticnet\"]),\n",
    "#     \"C\": hp.loguniform(\"C\", -4, 4),\n",
    "#     \"solver\": hp.choice(\"solver\", [\"saga\"]),  # Saga supports all penalties\n",
    "#     \"l1_ratio\": hp.uniform(\"l1_ratio\", 0, 1)  # Required for elasticnet\n",
    "# }\n",
    "\n",
    "# def objective(params, X, y, n_folds: int = N_FOLDS):\n",
    "#     \"\"\"Wrapper function for hyperparameter optimization\"\"\"\n",
    "#     try:\n",
    "#         # Handle elasticnet specific parameters\n",
    "#         if params[\"penalty\"] == \"elasticnet\":\n",
    "#             params[\"l1_ratio\"] = params.get(\"l1_ratio\", 0.5)\n",
    "#         else:\n",
    "#             params.pop(\"l1_ratio\", None)\n",
    "\n",
    "#         model = LogisticRegression(**params, max_iter=1000)\n",
    "#         scores = cross_validate(\n",
    "#             model, X, y,\n",
    "#             cv=n_folds,\n",
    "#             scoring=\"accuracy\",\n",
    "#             error_score=\"raise\"  # Get detailed errors\n",
    "#         )\n",
    "#         return {\n",
    "#             \"loss\": -np.mean(scores[\"test_score\"]),  # Minimize negative accuracy\n",
    "#             \"params\": params,\n",
    "#             \"status\": STATUS_OK\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         return {\n",
    "#             \"loss\": 0,\n",
    "#             \"status\": STATUS_FAIL,\n",
    "#             \"exception\": str(e)\n",
    "#         }\n",
    "\n",
    "# def train_model(X, y, model_name: str, logger):\n",
    "#     \"\"\"Complete training pipeline with error handling\"\"\"\n",
    "#     logger.info(\"Loading target encoder/decoder\")\n",
    "#     try:\n",
    "#         with open(os.path.join(MODEL_PATH, model_name, \"model_target_translator.pkl\"), \"rb\") as pkl:\n",
    "#             translator = pickle.load(pkl)\n",
    "\n",
    "#         y_train_enc = translator['encoder'].transform(y)\n",
    "\n",
    "#         logger.info(\"Starting hyperparameter optimization\")\n",
    "#         bayes_trials = Trials()\n",
    "\n",
    "#         best = fmin(\n",
    "#             fn=partial(objective, X=X, y=y_train_enc),\n",
    "#             space=SPACE,\n",
    "#             algo=tpe.suggest,\n",
    "#             max_evals=MAX_EVALS,\n",
    "#             trials=bayes_trials,\n",
    "#             show_progressbar=False\n",
    "#         )\n",
    "\n",
    "#         # Get best parameters from trials\n",
    "#         best_params = bayes_trials.best_trial[\"result\"][\"params\"]\n",
    "#         logger.info(f\"Best parameters: {best_params}\")\n",
    "\n",
    "#         # Train final model\n",
    "#         final_model = LogisticRegression(**best_params, max_iter=1000)\n",
    "#         final_model.fit(X, y_train_enc)\n",
    "\n",
    "#         # Save artifacts\n",
    "#         os.makedirs(os.path.join(MODEL_PATH, model_name), exist_ok=True)\n",
    "#         with open(os.path.join(MODEL_PATH, model_name, \"final_model.pkl\"), \"wb\") as pkl:\n",
    "#             pickle.dump(final_model, pkl)\n",
    "\n",
    "#         logger.info(\"Model trained and saved successfully\")\n",
    "\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Training failed: {str(e)}\")\n",
    "#         raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466cb72",
   "metadata": {},
   "source": [
    "### Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b19272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import pickle\n",
    "# from sklearn.metrics import classification_report\n",
    "# from skore import EstimatorReport\n",
    "\n",
    "# MODEL_PATH = \"models\"\n",
    "# REPORT_PATH = \"reports\"\n",
    "# def evaluate(X_test, y_test, model_name: str, logger):\n",
    "#     \"\"\"Proper evaluation function with correct encoding\"\"\"\n",
    "#     logger.info(\"Starting model evaluation\")\n",
    "\n",
    "#     try:\n",
    "#         # Load artifacts\n",
    "#         with open(os.path.join(MODEL_PATH, model_name, \"model_target_translator.pkl\"), \"rb\") as pkl:\n",
    "#             translator = pickle.load(pkl)\n",
    "\n",
    "#         with open(os.path.join(MODEL_PATH, model_name, \"final_model.pkl\"), \"rb\") as pkl:\n",
    "#             model = pickle.load(pkl)\n",
    "\n",
    "#         # Encode test labels\n",
    "#         y_test_enc = translator['encoder'].transform(y_test)\n",
    "\n",
    "#         # Generate predictions\n",
    "#         y_pred = model.predict(X_test)\n",
    "\n",
    "#         # Convert numeric class labels to strings\n",
    "#         class_names = [str(v) for v in translator['decoder'].values()]\n",
    "\n",
    "#         # Generate classification report\n",
    "#         evaluation_report = classification_report(\n",
    "#             y_test_enc,\n",
    "#             y_pred,\n",
    "#             target_names=class_names  # Use string labels\n",
    "#         )\n",
    "\n",
    "#         logger.info(\"saving evaluation report\")\n",
    "#         if not os.path.exists(os.path.join(REPORT_PATH, model_name)):\n",
    "#             os.makedirs(os.path.join(REPORT_PATH, model_name))\n",
    "#         with open(\n",
    "#             os.path.join(REPORT_PATH, model_name, \"evaluation_report.json\"), \"w\"\n",
    "#         ) as js:\n",
    "#             json.dump(evaluation_report, js, indent=4)\n",
    "#         logger.info(f\"Evaluation Report:\\n{evaluation_report}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Evaluation failed: {str(e)}\")\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac1617",
   "metadata": {},
   "source": [
    "### Trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b0d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logger import ExecutorLogger\n",
    "from src.training.process_data import read_process_data\n",
    "from src.training.inference.evaluate import evaluate\n",
    "from src.training.train import train_model, encode_target\n",
    "\n",
    "logger = ExecutorLogger(\"train pipeline\")\n",
    "\n",
    "\n",
    "def train_pipeline(logger: ExecutorLogger):\n",
    "    # process pipeline\n",
    "    numeric_features = [\"Age\", \"Fare\", \"SibSp\", \"Parch\"]\n",
    "    categorical_features = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "    drop_features = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"Survived\"]\n",
    "    logger.info(\"Training Started...\")\n",
    "    read_process_data(\n",
    "        file_name=\"titanic\",\n",
    "        target=\"Survived\",\n",
    "        num_cols=numeric_features,\n",
    "        cat_cols=categorical_features,\n",
    "        drop_cols=drop_features,\n",
    "        logger=logger,\n",
    "    )\n",
    "    X_train, y_train, X_test, y_test = encode_target(\n",
    "        file_name=\"titanic\",\n",
    "        target_col=\"Survived\",\n",
    "        model_name=\"basemodel\",\n",
    "        logger=logger,\n",
    "    )\n",
    "    train_model(X=X_train, y=y_train, model_name=\"basemodel\", logger=logger)\n",
    "    evaluate(X_test, y_test, \"basemodel\", logger)\n",
    "    logger.info(\"Training Completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2be3ff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-01 13:15:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mTraining Started...\u001b[0m\n",
      "\u001b[32m2025-05-01 13:15:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mStarting data processing\u001b[0m\n",
      "\u001b[32m2025-05-01 13:15:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRaw data loaded: (891, 12)\u001b[0m\n",
      "\u001b[32m2025-05-01 13:15:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mTrain/Test split: (712, 12)/(179, 12)\u001b[0m\n",
      "\u001b[32m2025-05-01 13:15:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mProcessing complete. Final shapes: Train (712, 11), Test (179, 11)\u001b[0m\n",
      "\u001b[32m2025-05-01 13:15:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFitting the encoder/decoder of target variable\u001b[0m\n",
      "\u001b[32m2025-05-01 13:15:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mNumber of classes: 2\u001b[0m\n",
      "\u001b[32m2025-05-01 13:15:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mencoder/decoder of target created successfully\u001b[0m\n",
      "\u001b[32m2025-05-01 13:15:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mencoder/decoder of target saved\u001b[0m\n",
      "\u001b[32m2025-05-01 13:15:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mLoading target encoder/decoder\u001b[0m\n",
      "\u001b[32m2025-05-01 13:15:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mStarting hyperparameter optimization\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelhakiem/ml_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/abdelhakiem/ml_env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-01 13:16:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBest parameters: {'C': 0.9699899225454556, 'penalty': 'l2', 'solver': 'saga'}\u001b[0m\n",
      "\u001b[32m2025-05-01 13:16:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mModel trained and saved successfully\u001b[0m\n",
      "\u001b[32m2025-05-01 13:16:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mStarting model evaluation\u001b[0m\n",
      "\u001b[32m2025-05-01 13:16:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1msaving evaluation report\u001b[0m\n",
      "\u001b[32m2025-05-01 13:16:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mEvaluation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.89      0.85       110\n",
      "           1       0.79      0.67      0.72        69\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.78      0.79       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\u001b[0m\n",
      "\u001b[32m2025-05-01 13:16:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mTraining Completed...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_pipeline(logger=ExecutorLogger(\"training logger\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
